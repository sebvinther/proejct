{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import hopsworks\n",
    "import hsfs\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler from scikit-learn\n",
    "import joblib\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Connecting to hopsworks\n",
    "api_key = os.environ.get('hopsworks_api')\n",
    "project = hopsworks.login(api_key_value=api_key)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "#Another connection to hopsworks\n",
    "api_key = os.getenv('hopsworks_api')\n",
    "connection = hsfs.connection()\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='tesla_stocks_fv',\n",
    "    version=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up train & test split dates\n",
    "train_start = \"2022-06-22\"\n",
    "train_end = \"2023-12-31\"\n",
    "\n",
    "test_start = '2024-01-01'\n",
    "test_end = \"2024-05-08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the train/test split on the feature view with the split dates\n",
    "feature_view.create_train_test_split(\n",
    "    train_start=train_start,\n",
    "    train_end=train_end,\n",
    "    test_start=test_start,\n",
    "    test_end=test_end,\n",
    "    data_format='csv',\n",
    "    coalesce= True,\n",
    "    statistics_config={'histogram':True,'correlations':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the split from feature view\n",
    "X_train, X_test, y_train, y_test = feature_view.get_train_test_split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting date into datetime\n",
    "X_train['date'] = pd.to_datetime(X_train['date']).dt.date\n",
    "X_test['date'] = pd.to_datetime(X_test['date']).dt.date\n",
    "X_train['date'] = pd.to_datetime(X_train['date'])\n",
    "X_test['date'] = pd.to_datetime(X_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 'ticker' column\n",
    "tickers = X_train[['ticker']]\n",
    "\n",
    "# Initializing OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fitting and transforming the 'ticker' column\n",
    "ticker_encoded = encoder.fit_transform(tickers)\n",
    "\n",
    "# Converting the encoded column into a DataFrame\n",
    "ticker_encoded_df = pd.DataFrame(ticker_encoded.toarray(), columns=encoder.get_feature_names_out(['ticker']))\n",
    "\n",
    "# Concatenating the encoded DataFrame with the original DataFrame\n",
    "X_train = pd.concat([X_train, ticker_encoded_df], axis=1)\n",
    "\n",
    "# Dropping the original 'ticker' column\n",
    "X_train.drop('ticker', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting X train after onehotencoding 'Ticker'\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same for X test as done to X train\n",
    "\n",
    "tickers = X_test[['ticker']]\n",
    "\n",
    "# Initializing OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fitting and transforming the 'ticker' column\n",
    "ticker_encoded_test = encoder.fit_transform(tickers)\n",
    "\n",
    "# Converting the encoded column into a DataFrame\n",
    "ticker_encoded_df_test = pd.DataFrame(ticker_encoded_test.toarray(), columns=encoder.get_feature_names_out(['ticker']))\n",
    "\n",
    "# Concatenating the encoded DataFrame with the original DataFrame\n",
    "X_test = pd.concat([X_test, ticker_encoded_df_test], axis=1)\n",
    "\n",
    "# Dropping the original 'ticker' column\n",
    "X_test.drop('ticker', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in MinMaxScaler to be used on the target variable 'open'\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting and transforming the 'open' column\n",
    "#y_train['open_scaled'] = scaler.fit_transform(y_train[['open']])\n",
    "#y_train.drop('open', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same to y_test as done to y_train \n",
    "#y_test['open_scaled'] = scaler.fit_transform(y_test[['open']])\n",
    "#y_test.drop('open', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function for the LSTM model\n",
    "def create_model(input_shape,\n",
    "                 LSTM_filters=64,\n",
    "                 dropout=0.1,\n",
    "                 recurrent_dropout=0.1,\n",
    "                 dense_dropout=0.5,\n",
    "                 activation='relu',\n",
    "                 depth=1):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    if depth > 1:\n",
    "        for i in range(1, depth):\n",
    "            # Recurrent layer\n",
    "            model.add(LSTM(LSTM_filters, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(LSTM_filters, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    # Fully connected layer\n",
    "    if activation == 'relu':\n",
    "        model.add(Dense(LSTM_filters, activation='relu'))\n",
    "    elif activation == 'leaky_relu':\n",
    "        model.add(Dense(LSTM_filters))\n",
    "        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(dense_dropout))\n",
    "\n",
    "    # Output layer for predicting one day forward\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "# As X_train['date'] column exists and is in datetime format, we're converting it\n",
    "X_train['year'] = X_train['date'].dt.year\n",
    "X_train['month'] = X_train['date'].dt.month\n",
    "X_train['day'] = X_train['date'].dt.day\n",
    "\n",
    "# Dropping the original date column\n",
    "X_train.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Converting dataframe to numpy array\n",
    "X_train_array = X_train.to_numpy()\n",
    "\n",
    "# Reshaping the array to have a shape suitable for LSTM\n",
    "X_train_array = np.expand_dims(X_train_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to numpy array\n",
    "X_train_array = X_train.values\n",
    "\n",
    "# Reshaping X_train_array to add a time step dimension\n",
    "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], 1, X_train_array.shape[1])\n",
    "\n",
    "# Assuming X_train_reshaped shape is now (374, 1, 5)\n",
    "input_shape = X_train_reshaped.shape[1:]\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model on the training dataset\n",
    "model.fit(X_train_reshaped, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As X_test['date'] column exists and is in datetime format, we're converting it\n",
    "X_test['year'] = X_test['date'].dt.year\n",
    "X_test['month'] = X_test['date'].dt.month\n",
    "X_test['day'] = X_test['date'].dt.day\n",
    "\n",
    "# Dropping the original date column\n",
    "X_test.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Converting dataframe to numpy array\n",
    "X_test_array = X_test.to_numpy()\n",
    "\n",
    "# Reshape the array to have a shape suitable for LSTM\n",
    "X_test_array = np.expand_dims(X_test_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting y_pred with X_test\n",
    "y_pred = model.predict(X_test_array)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conneting to hopsworks model registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE metric for filling the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse_metrics = {\"RMSE\": rmse}\n",
    "rmse_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the model schema\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema, output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a file colled 'stock_model'\n",
    "model_dir=\"stock_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model to hopsworks model registry\n",
    "#stock_pred_model = mr.tensorflow.create_model(\n",
    "#        name=\"stock_pred_model\",\n",
    "#        metrics= rmse_metrics,\n",
    "#        model_schema=model_schema,\n",
    "#        description=\"Stock Market TSLA Predictor from News Sentiment\",\n",
    "#    )\n",
    "\n",
    "#stock_pred_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_tensorflow_model(model, name, description, features, labels, metrics):\n",
    "    from hsml.schema import Schema\n",
    "    from hsml.model_schema import ModelSchema\n",
    "    import os\n",
    "    import joblib\n",
    "    import shutil\n",
    "\n",
    "    mr = project.get_model_registry()\n",
    "\n",
    "    model_dir= name + \"_model\"\n",
    "    if os.path.isdir(model_dir) == False:\n",
    "        os.mkdir(model_dir)\n",
    "    pickle= name + '_model.pkl'\n",
    "    # This will strip out the sml directory, copying only the files\n",
    "    #shutil.copytree(\"sml\", model_dir, dirs_exist_ok=True) #python 3.8+\n",
    "\n",
    "    joblib.dump(model, model_dir + \"/\" + pickle)\n",
    "\n",
    "    input_example = features.sample()\n",
    "    input_schema = Schema(features)\n",
    "    output_schema = Schema(labels)\n",
    "    model_schema = ModelSchema(input_schema, output_schema)\n",
    "\n",
    "    stock_pred_model = mr.tensorflow.create_model(\n",
    "        name=\"stock_pred_model\", \n",
    "        metrics=rmse_metrics,\n",
    "        model_schema=model_schema,\n",
    "        input_example=input_example, \n",
    "        description=description)\n",
    "\n",
    "    # Save all artifacts in the model directory to the model registry\n",
    "    stock_pred_model.save(model_dir)\n",
    "\n",
    "\n",
    "register_tensorflow_model(model, \"stock_prediction\", \"Stock Prediction\", X_train, y_train, rmse_metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
